{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOZQLTkt0wh2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be710c09-fca7-41c4-84c6-ed6e1c1d3b5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SHA-1 Times: [2.0036697387695314e-05, 2.1309852600097655e-05, 6.484508514404297e-05, 2.391815185546875e-05, 3.201007843017578e-05, 4.317760467529297e-05, 0.00022496700286865235, 0.00029923915863037107, 0.0012158632278442382, 0.002529306411743164, 0.011963372230529784, 0.014505686759948731, 0.17283499240875244, 0.7321088075637817, 1.5409749841690064]\n",
            "SHA-256 Times: [1.2273788452148437e-05, 1.2102127075195312e-05, 1.633167266845703e-05, 1.5201568603515625e-05, 2.55584716796875e-05, 4.003524780273437e-05, 0.00012983322143554686, 0.0002526235580444336, 0.0011946582794189454, 0.002792024612426758, 0.013013925552368164, 0.02456313610076904, 0.2844443464279175, 1.4054347324371337, 2.9528909492492676]\n"
          ]
        }
      ],
      "source": [
        "import hashlib\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "\n",
        "def create_mixed_content_file(size, filename):\n",
        "    \"\"\"Create a file with mixed content of the specified size\"\"\"\n",
        "    with open(filename, 'wb') as f:\n",
        "        # Text part\n",
        "        text_part_size = size // 3\n",
        "        text = ''.join(random.choices('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789', k=text_part_size)).encode()\n",
        "        f.write(text)\n",
        "\n",
        "        # Binary part\n",
        "        binary_part_size = size // 3\n",
        "        binary_bytes_written = 0\n",
        "        while binary_bytes_written < binary_part_size:\n",
        "            chunk_size = min(binary_part_size - binary_bytes_written, 64)\n",
        "            f.write(random.getrandbits(chunk_size * 8).to_bytes(chunk_size, 'big'))\n",
        "            binary_bytes_written += chunk_size\n",
        "\n",
        "        # Image part\n",
        "        image_part_size = size - text_part_size - binary_part_size\n",
        "        image = Image.new('RGB', (100, 100), color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)))\n",
        "        imgByteArr = io.BytesIO()\n",
        "        image.save(imgByteArr, format='PNG')\n",
        "        imgByteArr = imgByteArr.getvalue()[:image_part_size]\n",
        "        f.write(imgByteArr)\n",
        "\n",
        "def hash_time(hash_function_name, filename):\n",
        "    \"\"\"Measure the time taken to hash data in a file with the specified hash function\"\"\"\n",
        "    hash_function = hashlib.new(hash_function_name)\n",
        "    start_time = time.time()\n",
        "    with open(filename, 'rb') as f:\n",
        "        for chunk in iter(lambda: f.read(4096), b''):\n",
        "            hash_function.update(chunk)\n",
        "    end_time = time.time()\n",
        "    return end_time - start_time\n",
        "\n",
        "def test_hash_function_performance(hash_function_name, data_sizes, iterations=50):\n",
        "    \"\"\"Test the performance of a hash function for different data sizes and types\"\"\"\n",
        "    times = []\n",
        "\n",
        "    for size in data_sizes:\n",
        "        filename = f'temp_{size}.bin'\n",
        "        create_mixed_content_file(size, filename)\n",
        "        avg_time = np.mean([hash_time(hash_function_name, filename) for _ in range(iterations)])\n",
        "        times.append(avg_time)\n",
        "        os.remove(filename)\n",
        "    return times\n",
        "\n",
        "\n",
        "# Data sizes ranging from very small to extremely large\n",
        "data_sizes = [\n",
        "    10, 100, 500,                      # Very small files (10 bytes to 500 bytes)\n",
        "    1024, 5*1024, 10*1024,             # Small files (1 KB to 10 KB)\n",
        "    50*1024, 100*1024, 500*1024,       # Medium files (50 KB to 500 KB)\n",
        "    1024*1024, 5*1024*1024, 10*1024*1024,  # Larger files (1 MB to 10 MB)\n",
        "    100*1024*1024, 500*1024*1024,      # Very large files (100 MB to 500 MB)\n",
        "    1024*1024*1024                     # Extremely large file (1 GB)\n",
        "]\n",
        "\n",
        "\n",
        "# Test SHA-1 and SHA-256 performance\n",
        "sha1_times = test_hash_function_performance('sha1', data_sizes)\n",
        "sha256_times = test_hash_function_performance('sha256', data_sizes)\n",
        "\n",
        "# Print results (or plot them as needed)\n",
        "print(\"SHA-1 Times:\", sha1_times)\n",
        "print(\"SHA-256 Times:\", sha256_times)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTIUhNjs0wY5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data sizes in KB, MB, and GB for demonstration purposes\n",
        "data_sizes_kb = [10, 50, 100, 500, 1000]  # KB\n",
        "data_sizes_mb = [1, 5, 10, 50, 100]       # MB\n",
        "data_sizes_gb = [0.1, 0.5, 1]             # GB\n",
        "\n",
        "# Convert MB and GB to KB for uniformity\n",
        "data_sizes_mb_kb = [x * 1024 for x in data_sizes_mb]  # Convert MB to KB\n",
        "data_sizes_gb_kb = [x * 1024 * 1024 for x in data_sizes_gb]  # Convert GB to KB\n",
        "\n",
        "# Combine all sizes\n",
        "all_sizes_kb = data_sizes_kb + data_sizes_mb_kb + data_sizes_gb_kb\n",
        "\n",
        "# Creating subplots for different size ranges\n",
        "fig, axs = plt.subplots(3, 1, figsize=(7, 11))\n",
        "\n",
        "# Plot for KB sizes\n",
        "axs[0].plot(data_sizes_kb, sha1_times[:len(data_sizes_kb)], label='SHA-1', marker='o')\n",
        "axs[0].plot(data_sizes_kb, sha256_times[:len(data_sizes_kb)], label='SHA-256', marker='o')\n",
        "axs[0].set_title('Performance for KB Sized Data')\n",
        "axs[0].set_xlabel('Data Size (KB)')\n",
        "axs[0].set_ylabel('Average Time Taken (Seconds)')\n",
        "axs[0].legend()\n",
        "axs[0].grid(True)\n",
        "\n",
        "# Plot for MB sizes\n",
        "axs[1].plot(data_sizes_mb, sha1_times[len(data_sizes_kb):len(data_sizes_kb) + len(data_sizes_mb)], label='SHA-1', marker='o')\n",
        "axs[1].plot(data_sizes_mb, sha256_times[len(data_sizes_kb):len(data_sizes_kb) + len(data_sizes_mb)], label='SHA-256', marker='o')\n",
        "axs[1].set_title('Performance for MB Sized Data')\n",
        "axs[1].set_xlabel('Data Size (MB)')\n",
        "axs[1].set_ylabel('Average Time Taken (Seconds)')\n",
        "axs[1].legend()\n",
        "axs[1].grid(True)\n",
        "\n",
        "# Plot for GB sizes\n",
        "axs[2].plot(data_sizes_gb, sha1_times[-len(data_sizes_gb):], label='SHA-1', marker='o')\n",
        "axs[2].plot(data_sizes_gb, sha256_times[-len(data_sizes_gb):], label='SHA-256', marker='o')\n",
        "axs[2].set_title('Performance for GB Sized Data')\n",
        "axs[2].set_xlabel('Data Size (GB)')\n",
        "axs[2].set_ylabel('Average Time Taken (Seconds)')\n",
        "axs[2].legend()\n",
        "axs[2].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3u4Yfl9gUCLY",
        "outputId": "64cad0ce-c041-4bcd-b45c-f9f780207814"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File Size      T-statistic    P-value\n",
            "-----------  -------------  ---------\n",
            "KB                2.99816   0.0171195\n",
            "MB                0.470763  0.650379\n",
            "GB               -0.734605  0.503322\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Sample data for two conditions across different file sizes (KB, MB, GB)\n",
        "data_kb_condition1 = np.array(sha1_times[:len(data_sizes_kb)])  # KB data for condition 1\n",
        "data_kb_condition2 = np.array(sha256_times[:len(data_sizes_kb)])  # KB data for condition 2\n",
        "\n",
        "data_mb_condition1 = np.array(sha1_times[len(data_sizes_kb):len(data_sizes_kb) + len(data_sizes_mb)])  # MB data for condition 1\n",
        "data_mb_condition2 = np.array(sha256_times[len(data_sizes_kb):len(data_sizes_kb) + len(data_sizes_mb)])  # MB data for condition 2\n",
        "\n",
        "data_gb_condition1 = np.array(sha1_times[-len(data_sizes_gb):])  # GB data for condition 1\n",
        "data_gb_condition2 = np.array(sha256_times[-len(data_sizes_gb):])  # GB data for condition 2\n",
        "\n",
        "# Function to perform t-test and return results\n",
        "def perform_ttest(data1, data2):\n",
        "    t_stat, p_value = stats.ttest_ind(data1, data2)\n",
        "    return t_stat, p_value\n",
        "\n",
        "# Performing t-tests\n",
        "t_stat_kb, p_value_kb = perform_ttest(data_kb_condition1, data_kb_condition2)\n",
        "t_stat_mb, p_value_mb = perform_ttest(data_mb_condition1, data_mb_condition2)\n",
        "t_stat_gb, p_value_gb = perform_ttest(data_gb_condition1, data_gb_condition2)\n",
        "\n",
        "\n",
        "results_table = [\n",
        "    [\"KB\", t_stat_kb, p_value_kb],\n",
        "    [\"MB\", t_stat_mb, p_value_mb],\n",
        "    [\"GB\", t_stat_gb, p_value_gb]\n",
        "]\n",
        "\n",
        "print(tabulate(results_table, headers=[\"File Size\", \"T-statistic\", \"P-value\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGzmbZtaGLgN"
      },
      "source": [
        "# **Collision Experiment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7KXvfZlG-cT"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import random\n",
        "import hashlib\n",
        "\n",
        "def generate_random_input(length):\n",
        "    \"\"\"Generate a random string of specified length.\"\"\"\n",
        "    characters = string.ascii_letters + string.digits + string.punctuation\n",
        "    return ''.join(random.choice(characters) for i in range(length))\n",
        "\n",
        "def hash_input(hash_function_name, input_data):\n",
        "    \"\"\"Hash the input data using the specified hash function.\"\"\"\n",
        "    hash_function = hashlib.new(hash_function_name)\n",
        "    hash_function.update(input_data.encode())\n",
        "    return hash_function.hexdigest()\n",
        "\n",
        "def check_for_collisions(hash_function_name, inputs):\n",
        "    \"\"\"Check for collisions in hashed outputs.\"\"\"\n",
        "    hashed_outputs = set()\n",
        "    for input_data in inputs:\n",
        "        hash_output = hash_input(hash_function_name, input_data)\n",
        "        if hash_output in hashed_outputs:\n",
        "            return True, input_data  # Collision found\n",
        "        hashed_outputs.add(hash_output)\n",
        "    return False, None  # No collision found\n",
        "\n",
        "num_inputs = 10000000  # Number of unique inputs to generate\n",
        "input_length = 20  # Length of each input string\n",
        "\n",
        "# Generate unique inputs\n",
        "unique_inputs = [generate_random_input(input_length) for _ in range(num_inputs)]\n",
        "\n",
        "# Check for collisions in SHA-1\n",
        "collision_found_sha1, colliding_input_sha1 = check_for_collisions('sha1', unique_inputs)\n",
        "print(\"Collision in SHA-1:\", collision_found_sha1)\n",
        "\n",
        "# Check for collisions in SHA-256\n",
        "collision_found_sha256, colliding_input_sha256 = check_for_collisions('sha256', unique_inputs)\n",
        "print(\"Collision in SHA-256:\", collision_found_sha256)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hash Distribution**"
      ],
      "metadata": {
        "id": "7rkNU5S_Fwa9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFf-QM2TJ9Kx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_hash_distribution(hash_function_name, inputs, sample_size=num_inputs):\n",
        "    \"\"\"Plot the distribution of hash outputs for a sample of the inputs.\"\"\"\n",
        "    sample_hashes = [hash_input(hash_function_name, input_data)[:8] for input_data in random.sample(inputs, sample_size)]\n",
        "\n",
        "    # Convert hash outputs to integers for plotting\n",
        "    hash_values = [int(hash_output, 16) for hash_output in sample_hashes]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(hash_values, bins=50, color='skyblue', alpha=0.7)\n",
        "    plt.title(f'Hash Output Distribution for {hash_function_name.upper()}')\n",
        "    plt.xlabel('Hash Value (Truncated)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "plot_hash_distribution('sha1', unique_inputs)\n",
        "plot_hash_distribution('sha256', unique_inputs)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "rGzmbZtaGLgN"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}